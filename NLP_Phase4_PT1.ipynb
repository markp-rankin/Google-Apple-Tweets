{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports. Nothing to see here.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import string\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, plot_confusion_matrix, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "source": [
    "# EDA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Here we check out the data. We explore, look for inconsistencies, and use what we see to define our modeling plan. We start with general data cleaning practice, checking for duplicates and missing values. As we get deeper into the data, our techniques get more specific to the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_text</th>\n      <th>emotion_in_tweet_is_directed_at</th>\n      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n      <td>iPhone</td>\n      <td>Negative emotion</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n      <td>iPad or iPhone App</td>\n      <td>Positive emotion</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n      <td>iPad</td>\n      <td>Positive emotion</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@sxsw I hope this year's festival isn't as cra...</td>\n      <td>iPad or iPhone App</td>\n      <td>Negative emotion</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n      <td>Google</td>\n      <td>Positive emotion</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# Import and inspect data\n",
    "\n",
    "data = pd.read_csv('data.csv', encoding = \"ISO-8859-1\")\n",
    "raw = pd.read_csv('data.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Read content. These are a dataset of Tweets from SXSW in Austin from 2011.\n",
    "\n",
    "data.iloc[0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the primary column and remove the one we find.\n",
    "\n",
    "print(data['tweet_text'].isna().sum())\n",
    "\n",
    "data = data[~data['tweet_text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# Number of duplicate rows dropped: 22\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicated rows and preserve unique entries.\n",
    "\n",
    "a = len(data)\n",
    "data = data.drop_duplicates()\n",
    "b = len(data)\n",
    "print('# Number of duplicate rows dropped: {}'.format(a-b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "iPad                               945\nApple                              659\niPad or iPhone App                 469\nGoogle                             428\niPhone                             296\nOther Google product or service    293\nAndroid App                         80\nAndroid                             77\nOther Apple product or service      35\nName: emotion_in_tweet_is_directed_at, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Explore and simplify. We're defining our project goals more closely here.\n",
    "\n",
    "print(data['emotion_in_tweet_is_directed_at'].value_counts())\n",
    "\n",
    "company = {'iPad': 'Apple',\n",
    "            'Apple': 'Apple',\n",
    "            'iPad or iPhone App': 'Apple',\n",
    "            'Google': 'Google',\n",
    "            'iPhone': 'Apple',\n",
    "            'Other Google product or service': 'Google',\n",
    "            'Android App': 'Google',\n",
    "            'Android': 'Google',\n",
    "            'Other Apple product or service': 'Apple'}\n",
    "            \n",
    "data['emotion_in_tweet_is_directed_at'] = data['emotion_in_tweet_is_directed_at'].map(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify column names for convenience.\n",
    "\n",
    "data.rename(columns={'tweet_text': 'text', 'emotion_in_tweet_is_directed_at': 'brand', 'is_there_an_emotion_directed_at_a_brand_or_product': 'feelings'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original dataset values:\n No emotion toward brand or product    5375\nPositive emotion                      2970\nNegative emotion                       569\nI can't tell                           156\nName: feelings, dtype: int64 \n\nEncoded and chosen dataset values:\n 1    2970\n0     569\nName: feelings, dtype: int64 \n\nTotal entries: 3539\n"
     ]
    }
   ],
   "source": [
    "# Here we inspect and encode the labels for our target column. We also define the boundaries of our task, narrowing our dataset to entries with clear positive or negative expressions.\n",
    "\n",
    "print('Original dataset values:\\n', data['feelings'].value_counts(), '\\n')\n",
    "\n",
    "feels = {'Negative emotion': 0,\n",
    "        'Positive emotion': 1,\n",
    "        'No emotion toward brand or product': 2,\n",
    "        \"I can't tell\": 3}\n",
    "\n",
    "data['feelings'] = data['feelings'].map(feels)\n",
    "\n",
    "data = data[data['feelings'] <= 1]\n",
    "\n",
    "print('Encoded and chosen dataset values:\\n', data['feelings'].value_counts(), '\\n')\n",
    "\n",
    "print('Total entries:', len(data))"
   ]
  },
  {
   "source": [
    "# Now the NLP Begins"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Our data is clean and ready to be processed. Now we process. We start by creating tools, then use the tools on the Tweet text to reshape the data into a manageable and meaningful form."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tools to process the Tweets. We use Regex to narrow our data to words, and stopwords to return only the words that are significant.\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "stops += list(string.punctuation)\n",
    "stops.extend(['sxsw', 'sxswi', 'quot', 'mention', 'link', 'rt', 'amp', 'http', 'sxswrt', 'google', 'googles', 'app', 'apps', 'android', 'austin', 'quotgoogle', 'new', 'today', 'one', 'apple', 'ipad', 'iphone', 'ipad2', 'apples', 'quotapple','store'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our new tools!\n",
    "\n",
    "data['tokens'] = data['text'].apply(tokenizer.tokenize)\n",
    "\n",
    "# This is inefficient and can be worked to be better. First pass eliminates stopwords that are already lowercase, and turns the rest of the words lowercase. Second pass removes the remaining stopwords. It also leaves our data in list form, which can be an issue later. Our lemmatizing function is built to work with a list. We fix the issue before we Count Vectorize or TD-IDF in the main function below.\n",
    "data['tokens'] = data['tokens'].apply(lambda x: [word.lower() for word in x if word not in stops])\n",
    "data['tokens'] = data['tokens'].apply(lambda x: [word.lower() for word in x if word not in stops])\n",
    "\n",
    "data\n",
    "copy = data"
   ]
  },
  {
   "source": [
    "# Creating Functions for User Interface"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### It is important that we create functions for each of the modeling steps. We will be selecting which ones to use individually later and they all need to be defined before we know which ones will be used. It is extra important that they all work well individually and also do not interfere with each other."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorize or TF-IDF. Our first and most vital choice.\n",
    "def CV(X_train, X_test):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "    X_test_counts = count_vectorizer.transform(X_test)\n",
    "    return X_train_counts, X_test_counts\n",
    "\n",
    "def tf_idf(X_train, X_test):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    X_train_counts = tfidf.fit_transform(X_train)\n",
    "    X_test_counts = tfidf.transform(X_test)\n",
    "    return X_test_counts, X_train_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing will or won't happen. Two functions, one nested inside the other.\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "def lemmatize():\n",
    "    data['lemm'] = data['tokens'].apply(lemmatize_text)\n",
    "    data['lemm'] = data['lemm'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE either will or will not run.\n",
    "def smote(X_train_counts, y_train):\n",
    "    smote = SMOTE()\n",
    "    X_train_counts, y_train = smote.fit_sample(X_train_counts, y_train)\n",
    "    return X_train_counts, y_train\n",
    "\n",
    "# Train Test Split. The col variable is important and will be different depending on whether we lemmatize.\n",
    "def TTS(col):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[col], data['feelings'])\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def logreg(X_train_counts, y_train, X_test_counts):\n",
    "    clf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n",
    "                         multi_class='multinomial', n_jobs=-1, random_state=40)\n",
    "    clf.fit(X_train_counts, y_train)\n",
    "    y_predicted_counts = clf.predict(X_test_counts)\n",
    "    return y_predicted_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "def rf(X_train_counts, y_train, X_test_counts):\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train_counts, y_train)\n",
    "    y_predicted_counts = rf.predict(X_test_counts)\n",
    "    return y_predicted_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes\n",
    "def multiNB(X_train_counts, y_train, X_test_counts):\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_counts, y_train)\n",
    "    y_predicted_counts = nb.predict(X_test_counts)\n",
    "    return y_predicted_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics that will work with any model selected, along with a confusion matrix.\n",
    "def classify(y_test, y_predicted_counts):\n",
    "    print('\\n\\nClassification Report - TEST')\n",
    "    print('--------------------------------------------------------------------------')\n",
    "    print(classification_report(y_test, y_predicted_counts))\n",
    "    print('--------------------------------------------------------------------------')\n",
    "    print('Confusion Matrix - TEST')\n",
    "    print('--------------------------------------------------------------------------')\n",
    "    print(pd.crosstab(y_test, y_predicted_counts, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "    print('--------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Our Modeling System!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### We made the choice to create an interactive modeling system. The user will be asked to supply inputs and choose his own path to model the data. First, we choose to Count Vectorize or TF-IDF, whether or not to Lemmatize or to SMOTE. There are 8 possible combinations of choices, each accounted for. We've even processed every combination beforehand to have recommended modeling depending on the user's choices. Modeling is fun again!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoclean = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_models(data):\n",
    "    # Canned response for every response that isn't '1' or '2.'\n",
    "    jerk = \"Don't waste my time. Try again wiseguy.\"\n",
    "\n",
    "    # Prepare some variables.\n",
    "    col = None\n",
    "    model = None\n",
    "    lemm = None\n",
    "    smt = None\n",
    "    reco = None\n",
    "    picks = []\n",
    "    choices = [logreg, rf, multiNB]\n",
    "    t = None\n",
    "    proceed = 1\n",
    "\n",
    "    # User choices will influence recommended model selection.\n",
    "    log = [[1, 2, 2], [2,2,2]]\n",
    "    forest = [[1,1,2],[2,1,2]]\n",
    "    NB = [[1,2,1],[2,2,1],[1,1,1,],[2,1,1]]\n",
    "    \n",
    "    ################ For Advanced Interface #####################\n",
    "    print('THE DATA SCIENCE PROCESS! \\nCLEANING AND MODELING\\n')\n",
    "    print(\"Let's model our data! Where do you want to begin?\\n\\nType '1' start modeling our data.\\nType '2' if you want to be lazy.\\n\")\n",
    "    lazy = input()\n",
    "    if lazy not in ['1','2']:\n",
    "        print(jerk)\n",
    "    elif lazy == '2':\n",
    "        print(\"\\nYeah, I get it. I'll do all the work.\\n\")\n",
    "        lemmatize()\n",
    "        t = 2\n",
    "        col = 'lemm'\n",
    "        X_train, X_test, y_train, y_test = TTS(col)\n",
    "        X_test_counts, X_train_counts = tf_idf(X_train, X_test)\n",
    "        X_train_counts, y_train = smote(X_train_counts, y_train)\n",
    "        mod = choices[t]\n",
    "        print('-------------------------------------------------------------------------------------------------------------')\n",
    "        print('-------------------------------------------------------------------------------------------------------------')\n",
    "        print('Our best model uses TF-IDF, SMOTE, Lemmatization, and is created using Multinomial Naive Bayes, you lazy bum.')\n",
    "        print('We chose this model because of its high recall combined with a high overall accuracy.')\n",
    "        print('-------------------------------------------------------------------------------------------------------------')\n",
    "        print('-------------------------------------------------------------------------------------------------------------')\n",
    "        y_predicted_counts = mod(X_train_counts, y_train, X_test_counts)\n",
    "        classify(y_test, y_predicted_counts)\n",
    "        return\n",
    "\n",
    "    # Here we ask three questions, leading to 8 possible combinations. The answers affect what functions are run, and also are saved in a list and used to compare for model selection.\n",
    "    while proceed == 1:\n",
    "        print('How would you like to analyze the data?\\n\\nType \"1\" to Count Vectorize or \"2\" to implement TF-IDF.\\nIf you don\\'t know the difference, type \"3.\"\\n\\n')\n",
    "        model = input()\n",
    "        if int(model) < 3:\n",
    "            picks.append(int(model))\n",
    "        if model not in ['1', '2', '3']:\n",
    "            return print(jerk)\n",
    "        if model == '3':\n",
    "            print('\\nCount Vectorizing creates a matrix consisting of a count of individual tokens across a single data entry. \\nTerm Frequency-Inverse Document Frequency creates a matrix but assigns each token a weight according to its importance weighted by its frequency across documents.\\n')\n",
    "        else:\n",
    "            proceed = 0\n",
    "\n",
    "    while proceed == 0:\n",
    "        print('Would you like to lemmatize?\\n\\nType \"1\" for Yes or \"2\" for No.\\nIf you don\\'t know the difference, type \"3\"\\n\\n')\n",
    "        lemm  = input()\n",
    "        if int(lemm) < 3:\n",
    "            picks.append(int(lemm))\n",
    "        if lemm not in ['1', '2','3']:\n",
    "            return print(jerk)\n",
    "        elif lemm == '3':\n",
    "            print('\\nLemmatization transforms a word into its root form: i.e. running, ran, run all become the word run.\\n')\n",
    "        else:\n",
    "            proceed = 1\n",
    "\n",
    "    while proceed == 1:\n",
    "        print('Would you like to SMOTE?\\n\\nType \"1\" for Yes or \"2\" for No.\\nIf you don\\'t know the difference, type \"3\"\\n\\n')\n",
    "        smt = input()\n",
    "        if int(smt) < 3:\n",
    "            picks.append(int(smt))\n",
    "        if smt not in ['1','2','3']:\n",
    "            return print(jerk)\n",
    "        if smt == '3':\n",
    "            print('\\nSMOTE is used to address class imbalance. It creates artificial data in the test dataset similar to the qualitites of the minority class.\\n')\n",
    "        else:\n",
    "            proceed = 0\n",
    "\n",
    "    # Lemmatize function runs or not. The function requires the data to be in list form. The function will remove the list form.\n",
    "    # If not lemmatized, data['tokens'] column is taken out of list form.\n",
    "    if lemm == '1':\n",
    "        col = 'lemm'\n",
    "        lemmatize()\n",
    "        print('Lemmatized changed this many rows:', (len(data) - (sum(data['tokens'] == data['lemm']))),'\\n')\n",
    "    elif lemm == '2':\n",
    "        col = 'tokens'\n",
    "        data['tokens'] = data['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    # Here we compare the user's choices to our previously established lists. They are organized and hand picked from personal results to produce the best results.\n",
    "    if picks in log:\n",
    "        t = 0\n",
    "    elif picks in forest:\n",
    "        t = 1\n",
    "    elif picks in NB:\n",
    "        t = 2\n",
    "    print(picks)\n",
    "\n",
    "    # Train Test Split. Our column choice is based on lemmatization choice.\n",
    "    X_train, X_test, y_train, y_test = TTS(col)\n",
    "\n",
    "    # Count Vectorize or TF-IDF.\n",
    "    if model == '1':\n",
    "        X_train_counts, X_test_counts = CV(X_train, X_test)\n",
    "    elif model == '2':\n",
    "        X_test_counts, X_train_counts = tf_idf(X_train, X_test)\n",
    "\n",
    "    # SMOTE or not.\n",
    "    if smt == '1':\n",
    "        X_train_counts, y_train = smote(X_train_counts, y_train)\n",
    " \n",
    "    # Now that choices are made and we've picked our recommendation, we give the user a choice. They have control of the entire modeling process.\n",
    "    print('Would you like to pick your model or use our recomendation?\\nType \"1\" to choose or \"2\" to let us.\\n')\n",
    "    reco = input()\n",
    "    if reco not in ['1','2']:\n",
    "        return print(jerk)\n",
    "    elif reco == '1':\n",
    "        print('\\n\\n\"1\" for Logistic Regression\\n\"2\" for Random Forest\\n\"3\" for Multinomial Naive Bayes')\n",
    "        t = (int(input()) - 1)\n",
    "\n",
    "    # t variable is either recommended by us or chosen by user. It picks an option from a list of models.\n",
    "    mod = choices[t]\n",
    "    if mod == logreg:\n",
    "        print('\\n\\nLogistic Regression Report')\n",
    "    elif mod == rf:\n",
    "        print('\\n\\nRandom Forest Report')\n",
    "    elif mod == multiNB:\n",
    "        print('\\n\\nMultinomial Naive Bayes')\n",
    "\n",
    "    # We run the chosen model and print out metrics for evaluation, along with a confusion matrix.\n",
    "    y_predicted_counts = mod(X_train_counts, y_train, X_test_counts)\n",
    "    classify(y_test, y_predicted_counts)\n",
    "\n",
    "    data = copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "THE DATA SCIENCE PROCESS! \n",
      "CLEANING AND MODELING\n",
      "\n",
      "Let's model our data! Where do you want to begin?\n",
      "\n",
      "Type '1' start modeling our data.\n",
      "Type '2' if you want to be lazy.\n",
      "\n",
      "How would you like to analyze the data?\n",
      "\n",
      "Type \"1\" to Count Vectorize or \"2\" to implement TF-IDF.\n",
      "If you don't know the difference, type \"3.\"\n",
      "\n",
      "\n",
      "\n",
      "Count Vectorizing creates a matrix consisting of a count of individual tokens across a single data entry. \n",
      "Term Frequency-Inverse Document Frequency creates a matrix but assigns each token a weight according to its importance weighted by its frequency across documents.\n",
      "\n",
      "How would you like to analyze the data?\n",
      "\n",
      "Type \"1\" to Count Vectorize or \"2\" to implement TF-IDF.\n",
      "If you don't know the difference, type \"3.\"\n",
      "\n",
      "\n",
      "Would you like to lemmatize?\n",
      "\n",
      "Type \"1\" for Yes or \"2\" for No.\n",
      "If you don't know the difference, type \"3\"\n",
      "\n",
      "\n",
      "\n",
      "Lemmatization transforms a word into its root form: i.e. running, ran, run all become the word run.\n",
      "\n",
      "Would you like to lemmatize?\n",
      "\n",
      "Type \"1\" for Yes or \"2\" for No.\n",
      "If you don't know the difference, type \"3\"\n",
      "\n",
      "\n",
      "Would you like to SMOTE?\n",
      "\n",
      "Type \"1\" for Yes or \"2\" for No.\n",
      "If you don't know the difference, type \"3\"\n",
      "\n",
      "\n",
      "[2, 2, 2]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-6b136053ef3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0muser_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-bf77b0f0e5a7>\u001b[0m in \u001b[0;36muser_models\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mX_train_counts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'2'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0mX_test_counts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;31m# SMOTE or not.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-3f6c9bf4ded9>\u001b[0m in \u001b[0;36mtf_idf\u001b[1;34m(X_train, X_test)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mX_train_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mX_test_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX_test_counts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_counts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1839\u001b[0m         \"\"\"\n\u001b[0;32m   1840\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1841\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1842\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1127\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m   1130\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m   1131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "user_models(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Our Favorite Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### We took an extremely scientific approach to analyzing our models: Predict, Experiment, Observe, Record the Results. With 3 possible models, a choice of processing the data, a choice to lemmatize and a choice to SMOTE, we end up with 24 (3 x 2 x 2 x 2) unique models. Observing the dataset to be imbalanced in favor of positive sentiments, we decided to focus primarily on the recall of the minority class. In practical terms, we are attempting to identify as many unhappy users of Google and Apple products as we can and prefer to err on the side of classifying extra unhappy users as oppoosed to missing them. \n",
    "\n",
    "### We run all 24 models individually and record their most important metrics: Accuracy, Precision, Recall, and F1 Score. Looking at the spreadsheet of results, a few things jump out at us. Excepting several outliers of lower recall, most of our recall scores are in the 0.40 - 0.60 range. Thus, we decided to factor in an overall metric, Accuracy when making our decisions. In each combination we use these metrics to choose our best model. In many categories it was a close decision and could have easily been justified to go in another direction."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![spreadsheet](spreadsheetTop.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Our favorite model overall used Term Frequencyâ€“Inverse Document Frequency to transform our data, Lemmatization, Smoting, and used Multinomial Naive Bayes to model it. We used all the bells and whistles for our best result. It produced our third highest recall score, and a higher overall accuracy than the two models with higher recall. Again, it was a tough call and other model choices could have been justified just as well."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![spreadsheet](spreadBottom.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### As highlighted above, recall was high in this model. The confusion matrix shows 86 correctly identified unhappy users, 51 unhappy users missed, and 126 happy users identified as unhappy. 51 missed users is tied with one other model as the lowest result we found. 126 happy users misidentified as unhappy is high, higher than we would like, but with over 600 correctly identified and a low number of unhappy users missed, we have hit our project target. The goal was to find unhappy users and overall accuracy and we are willing to accept a higher number of incorrectly identified users as long as they are happy. In this scenario, we would use these results to make unhappy users' experience better and the happy users may become even more positive."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_pickle(\"./exp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                   text   brand  feelings  \\\n0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   Apple         0   \n1     @jessedee Know about @fludapp ? Awesome iPad/i...   Apple         1   \n2     @swonderlin Can not wait for #iPad 2 also. The...   Apple         1   \n3     @sxsw I hope this year's festival isn't as cra...   Apple         0   \n4     @sxtxstate great stuff on Fri #SXSW: Marissa M...  Google         1   \n...                                                 ...     ...       ...   \n9077  @mention your PR guy just convinced me to swit...   Apple         1   \n9079  &quot;papyrus...sort of like the ipad&quot; - ...   Apple         1   \n9080  Diller says Google TV &quot;might be run over ...  Google         0   \n9085  I've always used Camera+ for my iPhone b/c it ...   Apple         1   \n9088                      Ipad everywhere. #SXSW {link}   Apple         1   \n\n                                                 tokens  \n0     wesley83 3g 3 hrs tweeting rise_austin dead ne...  \n1     jessedee know fludapp awesome likely appreciat...  \n2                           swonderlin wait 2 also sale  \n3                        hope year festival crashy year  \n4     sxtxstate great stuff fri marissa mayer tim re...  \n...                                                 ...  \n9077  pr guy convinced switch back great coverage pr...  \n9079                 papyrus sort like nice lol lavelle  \n9080  diller says tv might run playstation xbox esse...  \n9085  always used camera b c image stabilizer mode s...  \n9088                                         everywhere  \n\n[3539 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}